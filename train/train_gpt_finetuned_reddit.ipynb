{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b0dcdf92",
      "metadata": {
        "id": "b0dcdf92"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import json\n",
        "# from model import BigramLanguageModel\n",
        "# from train_model import EncodingDecoding, save_encoder\n",
        "import sys\n",
        "import pathlib\n",
        "# parent_dir = pathlib.Path(__file__).resolve().parent.parent\n",
        "# models_path = str(parent_dir / \"models\")\n",
        "# train_path = str(parent_dir / \"train\")\n",
        "# if models_path not in sys.path:\n",
        "#     sys.path.insert(0, models_path)\n",
        "# if train_path not in sys.path:\n",
        "#     sys.path.insert(0, train_path)\n",
        "\n",
        "from GPTModel import *\n",
        "from train_model import ByteEncodingTokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "0e82cf22",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0e82cf22",
        "outputId": "1259cb21-9026-4ed6-8a7a-89fe50052e85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(1337)\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Hyperparameters based on device\n",
        "\n",
        "if device == 'cuda':\n",
        "    batch_size = 64\n",
        "    max_iteration = 5000\n",
        "    block_size = 256\n",
        "    learning_rate = 3e-4\n",
        "    eval_interval = 500\n",
        "    n_embed = 384\n",
        "    dropout = 0.2\n",
        "    n_head = 6\n",
        "    n_layer = 6\n",
        "    eval_iters = 200\n",
        "else:\n",
        "    batch_size = 32\n",
        "    max_iteration = 3000\n",
        "    block_size = 128\n",
        "    eval_iters = 100\n",
        "    learning_rate = 2e-4\n",
        "    eval_interval = 300\n",
        "    n_embed = 128\n",
        "    dropout = 0.1\n",
        "    n_head = 4\n",
        "    n_layer = 4\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "dac599f8",
      "metadata": {
        "id": "dac599f8"
      },
      "outputs": [],
      "source": [
        "file_path = \"takeTurnConv.json\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "172f3990",
      "metadata": {
        "id": "172f3990"
      },
      "source": [
        "Read the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "9d06f1e9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d06f1e9",
        "outputId": "1c6c674f-ed41-4519-83b6-09799ff9428f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total conversations: 14,408\n",
            "Example output: Human: What kind of phone(s) do you guys have?\n",
            "Assistant: I have a pixel. It's pretty great. Much better than what I had before.<|endoftext|>\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "\n",
        "EOS_TOKEN = \"<|endoftext|>\"\n",
        "\n",
        "conversations = []\n",
        "\n",
        "with open(file_path, 'r', encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        try:\n",
        "            data = json.loads(line)\n",
        "        except json.JSONDecodeError:\n",
        "            continue\n",
        "\n",
        "        messages = data.get('message', [])\n",
        "        if len(messages) != 2:\n",
        "            continue\n",
        "\n",
        "        user_msg = messages[0].get('content', '').strip()\n",
        "        assistant_msg = messages[1].get('content', '').strip()\n",
        "\n",
        "        if not user_msg or not assistant_msg:\n",
        "            continue\n",
        "\n",
        "        if user_msg.isnumeric() or assistant_msg.isnumeric():\n",
        "            continue\n",
        "\n",
        "        formatted = f\"Human: {user_msg}\\nAssistant: {assistant_msg}{EOS_TOKEN}\"\n",
        "        conversations.append(formatted)\n",
        "\n",
        "print(f\"Total conversations: {len(conversations):,}\")\n",
        "print(f\"Example output: {conversations[0]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "conversations[0]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "-ymcmqLVij1o",
        "outputId": "c764230b-3ad7-495d-dd56-56a5a1c0c91c"
      },
      "id": "-ymcmqLVij1o",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Human: What kind of phone(s) do you guys have?\\nAssistant: I have a pixel. It's pretty great. Much better than what I had before.<|endoftext|>\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e76a8f42",
      "metadata": {
        "id": "e76a8f42"
      },
      "source": [
        "Split Train and Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "cec9bed6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cec9bed6",
        "outputId": "87104f60-8479-4de4-8a40-4851b5c2abef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoded: [20, 43, 50, 50, 53, 1, 20, 59, 51, 39, 52]\n",
            "Decoded: Hello Human\n",
            "Vocab size: 85\n"
          ]
        }
      ],
      "source": [
        "from train_model import split_conversations\n",
        "\n",
        "full_text = \"\".join(conversations)\n",
        "tokenizer = ByteEncodingTokenizer()\n",
        "\n",
        "encoded = tokenizer.encode(\"Hello Human\")\n",
        "print(\"Encoded:\", encoded)\n",
        "\n",
        "decoded = tokenizer.decode(encoded)\n",
        "\n",
        "print(\"Decoded:\", decoded)\n",
        "\n",
        "print(\"Vocab size:\", tokenizer.vocab_size)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "2ee702c8",
      "metadata": {
        "id": "2ee702c8"
      },
      "outputs": [],
      "source": [
        "\n",
        "from train_model import split_conversations, ChatDataset\n",
        "train_hf_ds, val_hf_ds = split_conversations(conversations)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_text_list = train_hf_ds[\"text\"]\n",
        "val_text_list = val_hf_ds[\"text\"]\n",
        "\n",
        "print(f\"Training samples: {len(train_text_list):,}\")\n",
        "print(f\"Validation samples: {len(val_text_list):,}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aK1HsMhZt2pq",
        "outputId": "a725da67-585f-42b9-8aee-7bb4d408fdb2"
      },
      "id": "aK1HsMhZt2pq",
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training samples: 12,967\n",
            "Validation samples: 1,441\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = ChatDataset(train_text_list, tokenizer, block_size)\n",
        "val_dataset = ChatDataset(val_text_list, tokenizer, block_size)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(\"DataLoaders ready for Fine-Tuning!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ytoy6j0vubdt",
        "outputId": "c74bc4b0-675f-48ed-a257-d88c982fa4ae"
      },
      "id": "Ytoy6j0vubdt",
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataLoaders ready for Fine-Tuning!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ec90f2e1",
      "metadata": {
        "id": "ec90f2e1"
      },
      "source": [
        "Fine-tune the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "9f70deb4",
      "metadata": {
        "id": "9f70deb4"
      },
      "outputs": [],
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split, loader in [('train', train_loader), ('val', val_loader)]:\n",
        "        losses = []\n",
        "        for step, (X, Y) in enumerate(loader):\n",
        "            if step >= eval_iters: break\n",
        "\n",
        "            X, Y = X.to(device), Y.to(device)\n",
        "            logits, loss = model(X, targets=Y)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        out[split] = sum(losses) / len(losses)\n",
        "    model.train()\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from GPTModel import GPTModelStyle\n",
        "import torch.nn as nn\n",
        "\n",
        "model = GPTModelStyle(\n",
        "    vocab_size=65,\n",
        "    n_embed=n_embed,\n",
        "    block_size=block_size,\n",
        "    n_head=n_head,\n",
        "    n_layer=n_layer,\n",
        "    dropout=dropout,\n",
        "    device=device\n",
        ")\n",
        "model.to(device)\n",
        "\n",
        "model.load_state_dict(torch.load(\"gpt_pretrained_shakespeare.pt\", map_location=device))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIb4ECHavGrv",
        "outputId": "22edddab-69cc-4ec5-cb76-593cf147a834"
      },
      "id": "sIb4ECHavGrv",
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "current_vocab_size = model.token_embedding_table.weight.shape[0]\n",
        "\n",
        "if current_vocab_size < tokenizer.vocab_size:\n",
        "    print(f\"Resizing model from {current_vocab_size} to {tokenizer.vocab_size}...\")\n",
        "    num_new = tokenizer.vocab_size - current_vocab_size\n",
        "    old_embeddings = model.token_embedding_table.weight.data\n",
        "    new_embeddings = torch.randn(num_new, n_embed, device=device) * 0.02\n",
        "\n",
        "    model.token_embedding_table = nn.Embedding.from_pretrained(\n",
        "        torch.cat((old_embeddings, new_embeddings), dim=0),\n",
        "        freeze=False\n",
        "    )\n",
        "    old_head_w = model.lm_head.weight.data\n",
        "    new_head_w = torch.randn(num_new, n_embed, device=device) * 0.02\n",
        "    final_head_w = torch.cat((old_head_w, new_head_w), dim=0)\n",
        "    if model.lm_head.bias is not None:\n",
        "        old_head_b = model.lm_head.bias.data\n",
        "        new_head_b = torch.zeros(num_new, device=device)\n",
        "        final_head_b = torch.cat((old_head_b, new_head_b), dim=0)\n",
        "        has_bias = True\n",
        "    else:\n",
        "        has_bias = False\n",
        "\n",
        "\n",
        "    new_head = nn.Linear(n_embed, tokenizer.vocab_size, bias=has_bias)\n",
        "    new_head.weight = nn.Parameter(final_head_w)\n",
        "    if has_bias:\n",
        "        new_head.bias = nn.Parameter(final_head_b)\n",
        "\n",
        "    new_head.to(device)\n",
        "    model.lm_head = new_head\n",
        "\n",
        "    model.vocab_size = tokenizer.vocab_size\n",
        "    print(f\"Surgery complete. Model vocab size is now {model.vocab_size}.\")\n",
        "else:\n",
        "    print(\"Model is already resized.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JHo300P41Djl",
        "outputId": "a6960f14-1688-48d6-8881-e62543df2328"
      },
      "id": "JHo300P41Djl",
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Resizing model from 65 to 85...\n",
            "Surgery complete. Model vocab size is now 85.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4e60e66a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e60e66a",
        "outputId": "a7e79d28-958f-4e3c-f894-5363fa41bf20"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting fine-tuning for up to 5000 iterations...\n",
            "Block size: 256, Batch size: 64, LR: 0.0003\n",
            "------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-2)\n",
        "\n",
        "print(f\"\\nStarting fine-tuning for up to {max_iteration} iterations...\")\n",
        "print(f\"Block size: {block_size}, Batch size: {batch_size}, LR: {learning_rate}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "best_val_loss = float('inf')\n",
        "patience = 5\n",
        "patience_counter = 0\n",
        "\n",
        "train_iter = iter(train_loader)\n",
        "\n",
        "for iteration in range(max_iteration):\n",
        "    if iteration % eval_interval == 0 or iteration == max_iteration - 1:\n",
        "        losses = estimate_loss()\n",
        "        val_loss = losses['val']\n",
        "        print(f\"Step {iteration:5d}: train loss {losses['train']:.4f}, val loss {val_loss:.4f}\")\n",
        "\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            patience_counter = 0\n",
        "\n",
        "            torch.save(model.state_dict(), \"best_chat_model.pt\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "            if patience_counter >= patience:\n",
        "                print(f\"\\nEarly stopping triggered at step {iteration}. Val loss did not improve.\")\n",
        "                break\n",
        "\n",
        "    try:\n",
        "        xb, yb = next(train_iter)\n",
        "    except StopIteration:\n",
        "        train_iter = iter(train_loader)\n",
        "        xb, yb = next(train_iter)\n",
        "    xb, yb = xb.to(device), yb.to(device)\n",
        "\n",
        "    logits, loss = model(xb, targets=yb)\n",
        "\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(\"\\nTraining complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebab9b62",
      "metadata": {
        "id": "ebab9b62"
      },
      "outputs": [],
      "source": [
        "model_path = \"GPT-FineTunned.pt\"\n",
        "torch.save(model.state_dict(), model_path)\n",
        "print(f\"Model saved to {model_path}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"Quick generation test:\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "model.eval()\n",
        "context = torch.tensor([tokenizer.encode(\"Human: Hello!\\nAssistant: \")], dtype=torch.long, device=device)\n",
        "\n",
        "with torch.no_grad():\n",
        "    generated = model.generate(context, max_new_tokens=100)\n",
        "\n",
        "result = tokenizer.decode(generated[0].tolist())\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2793d116"
      },
      "source": [
        "# Task\n",
        "Adjust the model's `lm_head` (final output layer) to match the `current_vocab_size` (85) derived from the tokenizer, copy the weights for existing tokens, and initialize new weights for added tokens. Then, restart the fine-tuning process. Finally, save the fine-tuned model and perform a quick generation test."
      ],
      "id": "2793d116"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "622a176e"
      },
      "source": [
        "## Adjust Model Vocabulary\n",
        "\n",
        "### Subtask:\n",
        "Modify the code to ensure that the model's final output layer (lm_head) is correctly resized to match the `current_vocab_size` (85) derived from the tokenizer. This will involve re-initializing the lm_head layer with the new vocabulary size, copying the weights for existing tokens, and initializing new weights for any added tokens.\n"
      ],
      "id": "622a176e"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51f5833d"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask requires modifying the model's `lm_head` layer to match the new vocabulary size. I will locate the existing code where the `token_embedding_table` is resized and insert the logic for `lm_head` resizing immediately after it, following the provided instructions.\n",
        "\n"
      ],
      "id": "51f5833d"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}